{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Latent Dirichlet Allocation of Yoga Reviews\n",
    "\n",
    "Analysis flow in this notebook:\n",
    "\n",
    "* [Set analysis options](#section2)\n",
    "* [Analyze NYC reviews](#section3)\n",
    "* [Analyze LA reviews](#section4)\n",
    "    \n",
    "The analysis procedure itself consists of the following steps, separately applied to the NYC and LA corpora:\n",
    " 1. Concatenate the reviews by yoga business, making sure there are no duplicate reviews for a given business.\n",
    " 2. Convert to lower case, remove accents, tokenize, and retain only tokens with alphabetical characters.\n",
    " 3. Remove stop words and proper nouns.\n",
    " 4. Stem.\n",
    " 5. Create a corpus dictionary: (integer word ID, word, word frequency in corpus).\n",
    " 6. Remove tokens that appear too often or not often enough.\n",
    " 7. Convert each concatenated studio review into bag-of-words format: a list of (token ID, token count) 2-tuples.\n",
    " 8. Apply tf-idf transformation to corpus.\n",
    " 9. Apply Latent Dirichlet Allocation to corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "First get the packages we'll need.\n",
    "'''\n",
    "from   pymongo import MongoClient\n",
    "import logging\n",
    "import nltk\n",
    "from   gensim import corpora, models, similarities, matutils, utils\n",
    "from   collections import defaultdict\n",
    "from   pprint import pprint\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "[Back to top](#section1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Analysis Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stopwords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', \"'s\", \"n't\", \"'m\", 'ya', \"'ve\", 'also', 've', 'm']\n",
      "\n",
      "Number of proper nouns in removal list = 339\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Set common options for the LDA transformation that will be applied separately\n",
    "to the NYC and LA corpora.\n",
    "'''\n",
    "\n",
    "# SVD algorithm is stochastic.  This does not affect the results of the analysis, except for some \n",
    "# plots (since the overall sign of singular vectors can flip depending on the random number).\n",
    "# To avoid this problem, we set the seed here.\n",
    "np.random.seed(918273646)\n",
    "\n",
    "# The following are options for tokenizing, stemming, filtering, etc.\n",
    "convert_lowercase = True\n",
    "remove_accents    = True\n",
    "remove_stopwords  = True\n",
    "remove_proper     = True # If you enable this, make sure convert_lowercase = True.\n",
    "stem_tokens       = True\n",
    "filter_tails      = True\n",
    "filter_low        = 6   # No dictionary entry for tokens that appear in 5 documents or less.\n",
    "filter_up         = 0.1 # No dictionary entry for tokens that appear in more than 5% of the corpus documents.\n",
    "n_LDA_topics      = 10\n",
    "\n",
    "# Make list of stopwords.\n",
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "stoplist.append(u'\\u0027s')   # \"'s\" as in \"he's\"\n",
    "stoplist.append(u'n\\u0027t')  # \"n't\" as in \"he hasn't\"\n",
    "stoplist.append(u'\\u0027m')   # \"'m\" as in \"I'm\"\n",
    "stoplist.append(u'ya')        # as in \"you\"\n",
    "stoplist.append(u'\\u0027ve')  # \"'ve\" as in \"I've\"\n",
    "stoplist.append(u'also')\n",
    "stoplist.append(u've')\n",
    "stoplist.append(u'm')\n",
    "print('List of stopwords: %s' %[str(word) for word in stoplist])\n",
    "\n",
    "# Make list of proper nouns.\n",
    "ppn = [\"aaron\", \"aarona\", \"abigail\", \"adam\", \"adelaide\", \"alan\", \"alexandra\", \"alexis\", \"alice\", \"alicia\", \n",
    "       \"alison\", \"alli\", \"allie\", \"ally\", \"alyssa\", \"amalia\", \"amanda\", \"amber\", \"ami\", \"amy\", \"ana\", \"andrea\", \n",
    "       \"angela\", \"angie\", \"anna\", \"anne\", \"annie\", \"anthony\", \"anya\", \"ariel\", \"ash\", \"ashley\", \"audra\", \n",
    "       \"becker\", \"becky\", \"belle\", \"beth\", \"beverly\", \"bijorn\", \"bjorn\", \"bonni\", \"brad\", \"brandi\", \"brandon\", \n",
    "       \"brian\", \"brittani\", \"brittany\", \"brook\", \n",
    "       \"caprice\", \"cara\", \"carey\", \"carla\", \"carlos\", \"carmen\", \"carolyn\", \"cassie\", \"cathy\", \"charlotte\", \n",
    "       \"chris\", \"chrissy\", \"christian\", \"christina\", \"christine\", \"claire\", \"claudia\", \"connie\", \"corey\", \n",
    "       \"courtney\", \"dad\", \"dalton\", \"dana\", \"daniel\", \"daniela\", \"danny\", \"davey\", \"david\", \"dawn\", \"deborah\", \n",
    "       \"deena\", \"diane\", \"dina\", \"dr\", \n",
    "       \"eddie\", \"edwin\", \"elaine\", \"elena\", \"elizabeth\", \"ella\", \"ellen\", \"emily\", \"eric\", \"erica\", \"erik\", \n",
    "       \"erika\", \"erin\", \"evans\", \"ezmy\", \"fergus\", \"fern\", \"francisco\", \"frank\", \"fred\", \n",
    "       \"gabriel\", \"gabriella\", \"gavin\", \"geralyn\", \"ghylian\", \"gigi\", \"gina\", \"glenda\", \n",
    "       \"hannah\", \"harper\", \"heather\", \"heidy\", \"henry\", \"hermann\", \"hermosa\", \"howard\", \"hsiao\", \"hunt\", \n",
    "       \"ikaika\", \"ingrid\", \"ivette\", \"jackie\", \"jacqui\", \"jahaira\", \"jake\", \"james\", \"jami\", \"jamie\", \"jane\", \n",
    "       \"janet\", \"jai\", \"jason\", \"jc\", \"jeff\", \"jen\", \"jeni\", \"jenni\", \"jennie\", \n",
    "       \"jennifer\", \"jenny\", \"jerry\", \"jess\", \"jesse\", \"jessica\", \"jill\", \"jillian\", \"jim\", \"joe\", \"joetta\", \n",
    "       \"john\", \"johnson\", \"joi\", \n",
    "       \"jose\", \"joseph\", \"josh\", \"joy\", \"joyce\", \"jq\", \"judy\", \"julia\", \"juliana\", \"julie\", \"justin\",\n",
    "       \"kalie\", \"kallie\", \"karen\", \"kari\", \"kathleen\", \"kate\", \"kathi\", \"katie\", \"kaurwar\", \"kelli\", \"kelly\", \n",
    "       \"ken\", \"kerri\", \"kerry\", \"kim\", \"kimberli\", \"kimmy\", \"kris\", \"kristen\",\n",
    "       \"lalita\", \"lance\", \"lani\", \"lara\", \"laura\", \"lauren\", \"laurie\", \"les\", \"lil\", \"liliana\", \"lilly\", \n",
    "       \"linda\", \"lindsay\", \"lindsey\", \"lisa\", \"liz\", \"lori\", \"luisa\", \"lulu\", \"lynn\", \"ma\", \n",
    "       \"madalina\", \"madison\", \"maggie\", \"mai\", \"malaika\", \"mandy\", \"marco\", \"margaret\", \"marja\", \"mark\", \n",
    "       \"marnie\", \"martha\", \"mary\", \"masako\", \"matt\", \"matthew\", \"mayuri\", \"meagan\", \"megan\", \"melissa\", \n",
    "       \"melody\", \"meredith\", \"meriany\", \"merilynn\", \"mia\", \"michael\", \"michelle\", \"mike\", \"mimi\", \"mollie\", \n",
    "       \"molly\", \"mommy\", \"monica\", \"monika\", \"morgan\", \"namgyal\", \"nancy\", \"naomi\", \"narisara\", \"natalie\", \n",
    "       \"nathaniel\", \"ness\", \"nick\", \"nicola\", \"nicole\", \"nikki\", \"novak\", \"patrick\", \"paula\", \"pauline\", \n",
    "       \"peter\", \"phebe\", \"phoenix\", \"politeia\", \"rachel\", \"rafael\", \"ramit\", \"rebeca\", \"rebecca\", \"renee\", \n",
    "       \"richard\", \"rob\", \"roger\", \"rose\", \"rosie\", \"rudy\", \"ruthie\", \"ryan\", \"sam\", \"samantha\", \n",
    "       \"sandhya\", \"santoshi\", \"sara\", \"sarah\", \"scott\", \"sean\", \"shana\", \"shawn\", \"shelly\", \"sherica\", \n",
    "       \"sherry\", \"sheryl\", \"sonia\", \"sonja\", \"spencer\", \"stacey\", \"stacy\", \"stefani\", \"stephan\", \"stephen\",\n",
    "       \"stephanie\", \"stephaine\", \"steve\", \"sue\", \"susan\", \"suzanne\", \"suzi\", \"suzie\", \"tarzana\", \"theresa\", \n",
    "       \"tiffani\", \"travis\", \"tzaki\", \"tsewang\", \"ty\", \"victoria\", \"vincent\", \"wayne\", \"wendi\", \"wendy\", \n",
    "       \"wesley\", \"zander\", \"yvonne\", \"zoe\", \n",
    "       \"westchester\", \"miami\", \"harlem\", \"hoboken\", \"soho\", \"washington\", \"tribeca\", \"montclair\", \"jersey\",\n",
    "       \"ues\", \"downtown\", \"blvd\", \"flatiron\", \"silverlake\", \"verdes\", \"segundo\", \"palisades\", \"venice\",\n",
    "       \"noho\", \"malibu\"]\n",
    "print\n",
    "print('Number of proper nouns in removal list = %i' % len(ppn))\n",
    "\n",
    "# Select the stemmer.\n",
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "[Back to top](#section1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze NYC Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening NYC database...\n",
      "Total number of Yoga businesses = 796\n",
      "Business Yoga to the People has 331 reviews\n",
      "Number of reviewed NYC Yoga businesses = 560\n",
      "Dictionary(3193 unique tokens: [u'dissatisfi', u'profici', u'foul', u'sleek', u'secondli']...)\n",
      "In NYC corpus: Number of terms = 3193, documents = 560, latent topics = 10\n",
      "CPU times: user 4min 49s, sys: 11 s, total: 5min\n",
      "Wall time: 4min 59s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "* Make a list of the NYC reviews we'll be analyzing, concatenating by business.\n",
    "* Tokenize, and if enabled, remove stopwords and proper nouns, and stem.\n",
    "* Create a corpus dictionary: (integer word ID, word, word frequency in corpus),\n",
    "  removing words that appear too infrequently or too frequently.\n",
    "* Convert the tokenized reviews of the corpus to bags of words.\n",
    "* Apply tf-idf transformation to corpus.\n",
    "* Apply Latent Dirichlet Allocation to the corpus.\n",
    "'''\n",
    "\n",
    "# Initialize output lists:\n",
    "NYC_names    = []\n",
    "NYC_reviews  = []\n",
    "NYC_ratings  = []\n",
    "NYC_websites = []\n",
    "\n",
    "client = MongoClient()\n",
    "yoga   = client.dsbc.yyrnyc\n",
    "print('Opening NYC database...')\n",
    "print('Total number of Yoga businesses = %i' %yoga.count())\n",
    "\n",
    "cursor = yoga.find()\n",
    "for record in cursor:\n",
    "    reviews = []\n",
    "    for review in record[\"usr_text\"]:\n",
    "        if review:\n",
    "            reviews.append(review)\n",
    "\n",
    "    # Eliminate duplicate reviews for a given studio\n",
    "    # (different studios may still \"share\" a review):\n",
    "    n_reviews = len(reviews)\n",
    "    ureviews  = []\n",
    "    for review in set(reviews):\n",
    "        ureviews.append(review)\n",
    "    n_ureviews = len(ureviews)\n",
    "    if n_ureviews > 300:\n",
    "        print('Business %s has %i reviews' %(record[\"biz_name\"],n_ureviews))\n",
    "    \n",
    "    # Concatenate the unique reviews by business.\n",
    "    con_review = \"\"\n",
    "    for review in ureviews:\n",
    "        con_review += \" \" + review\n",
    "\n",
    "    if con_review:\n",
    "        con_review += \" \" + record[\"biz_name\"] # Add business name string only if there are reviews\n",
    "        studio = record[\"biz_name\"]+\" [at] \"+record[\"biz_address\"]\n",
    "        NYC_names.append(studio)\n",
    "        NYC_reviews.append(con_review)\n",
    "        NYC_ratings.append(record[\"biz_rating\"])\n",
    "        NYC_websites.append(record[\"biz_website\"])\n",
    "\n",
    "print('Number of reviewed NYC Yoga businesses = %i' % len(NYC_reviews))\n",
    "\n",
    "# Tokenize, removing punctuation and numbers, and if enabled, convert to lower case and remove accents.\n",
    "NYC_reviews_1 = [list(utils.tokenize(studio_review,lowercase=convert_lowercase,\n",
    "                 deacc=remove_accents)) for studio_review in NYC_reviews]\n",
    "\n",
    "if remove_stopwords:\n",
    "    NYC_reviews_2 = [[word for word in studio_review if word not in stoplist] \n",
    "                     for studio_review in NYC_reviews_1]\n",
    "    NYC_reviews_1 = NYC_reviews_2[:]\n",
    "\n",
    "if remove_proper:\n",
    "    NYC_reviews_2 = [[word for word in studio_review if word not in ppn] \n",
    "                     for studio_review in NYC_reviews_1]\n",
    "    NYC_reviews_1 = NYC_reviews_2[:]\n",
    "\n",
    "if stem_tokens:\n",
    "    NYC_reviews_2 = [[stemmer.stem(word) for word in studio_review] for studio_review in NYC_reviews_1]\n",
    "    # Create a dictionary to map stems to words (this is a one-to-many map, but this shouldn't matter much).\n",
    "    NYC_stem_to_word = defaultdict(str)\n",
    "    for review in NYC_reviews_1:\n",
    "        for word in review:\n",
    "            word_stem = stemmer.stem(word)\n",
    "            NYC_stem_to_word[word_stem] = word    \n",
    "    NYC_reviews_1 = NYC_reviews_2[:]\n",
    "else:\n",
    "    NYC_stem_to_word = defaultdict(str)\n",
    "    for review in NYC_reviews_1:\n",
    "        for word in review:\n",
    "            NYC_stem_to_word[word] = word        \n",
    "        \n",
    "NYC_dictionary = corpora.Dictionary( NYC_reviews_1 )\n",
    "if filter_tails:\n",
    "    NYC_dictionary.filter_extremes( no_below=filter_low, no_above=filter_up, keep_n=None )\n",
    "print(NYC_dictionary)\n",
    "n_terms  = len(NYC_dictionary)\n",
    "n_docs   = len(NYC_reviews_1)\n",
    "print('In NYC corpus: Number of terms = %i, documents = %i, latent topics = %i' % (n_terms,n_docs,n_LDA_topics))\n",
    "\n",
    "NYC_corpus_bow = [NYC_dictionary.doc2bow(studio_review) for studio_review in NYC_reviews_1]\n",
    "\n",
    "NYC_tfidf = models.TfidfModel(NYC_corpus_bow)\n",
    "NYC_corpus_tfidf = NYC_tfidf[NYC_corpus_bow]\n",
    "\n",
    "NYC_lda_file = 'results/yoga_studios_nyc_lda'+str(n_LDA_topics)\n",
    "%time NYC_lda = models.LdaModel( NYC_corpus_tfidf, id2word=NYC_dictionary, num_topics=n_LDA_topics, \\\n",
    "                                 update_every=5, passes=100 )\n",
    "NYC_lda.save(NYC_lda_file)\n",
    "    \n",
    "NYC_corpus_lda = NYC_lda[NYC_corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  0.0065*blissful + 0.0041*insight + 0.0029*therapy + 0.0027*mature + 0.0026*temple + 0.0024*heartbeat + 0.0022*midnight + 0.0021*motion + 0.0020*stretchy + 0.0019*shown + 0.0019*profusely + 0.0019*dogma + 0.0018*retreat + 0.0017*arranging + 0.0017*split + 0.0017*depth + 0.0016*laughter + 0.0016*counseling + 0.0016*outdoor + 0.0016*chosen\n",
      "\n",
      "Probability displayed/total = 0.046583/1.000000\n",
      "\n",
      "1:  0.0037*tree + 0.0030*nidra + 0.0030*beneficial + 0.0026*farther + 0.0025*industrial + 0.0025*remaining + 0.0024*cosy + 0.0023*kirtan + 0.0022*accomodating + 0.0020*demeanor + 0.0020*healer + 0.0019*dig + 0.0019*swear + 0.0019*bottom + 0.0018*belly + 0.0018*friendships + 0.0017*jackson + 0.0017*modify + 0.0016*sued + 0.0016*meetup\n",
      "\n",
      "Probability displayed/total = 0.043855/1.000000\n",
      "\n",
      "2:  0.0049*zumba + 0.0038*barre + 0.0032*donations + 0.0026*pregnancy + 0.0026*yogaworks + 0.0025*height + 0.0024*children + 0.0023*hidden + 0.0022*ashtanga + 0.0020*treatment + 0.0020*lululemon + 0.0020*integrity + 0.0019*hatha + 0.0019*fusion + 0.0019*exhale + 0.0019*crunch + 0.0019*contractions + 0.0019*kundalini + 0.0019*cycle + 0.0018*yttp\n",
      "\n",
      "Probability displayed/total = 0.047558/1.000000\n",
      "\n",
      "3:  0.0066*crunch + 0.0065*carpeting + 0.0051*facial + 0.0040*pregnant + 0.0039*dahn + 0.0036*treadmills + 0.0031*brain + 0.0030*sauna + 0.0030*chi + 0.0029*tai + 0.0029*foundations + 0.0028*anatomy + 0.0028*pool + 0.0028*bronx + 0.0027*products + 0.0025*equinox + 0.0025*oils + 0.0025*holistic + 0.0025*expert + 0.0025*guests\n",
      "\n",
      "Probability displayed/total = 0.068321/1.000000\n",
      "\n",
      "4:  0.0103*tara + 0.0028*reiki + 0.0026*bonding + 0.0026*accident + 0.0023*car + 0.0023*apprehensive + 0.0023*metered + 0.0021*kristin + 0.0021*invigorated + 0.0021*regain + 0.0019*comparison + 0.0019*necessary + 0.0019*retired + 0.0018*remaining + 0.0018*hadn + 0.0017*aura + 0.0016*facebook + 0.0016*ailments + 0.0015*county + 0.0015*kula\n",
      "\n",
      "Probability displayed/total = 0.048531/1.000000\n",
      "\n",
      "5:  0.0054*reformer + 0.0053*therapy + 0.0042*y + 0.0039*thai + 0.0038*island + 0.0038*journey + 0.0038*therapist + 0.0037*powerflow + 0.0036*dark + 0.0035*structured + 0.0033*emotional + 0.0033*shanti + 0.0032*aerial + 0.0030*lotus + 0.0030*equally + 0.0029*terrific + 0.0029*tools + 0.0029*pounds + 0.0028*beach + 0.0028*appearance\n",
      "\n",
      "Probability displayed/total = 0.070918/1.000000\n",
      "\n",
      "6:  0.0062*coach + 0.0061*crossfit + 0.0031*fire + 0.0026*december + 0.0021*pregnancy + 0.0021*herbal + 0.0020*outfit + 0.0020*daughter + 0.0019*chiseling + 0.0019*ensure + 0.0019*intelligent + 0.0019*precious + 0.0018*centre + 0.0018*profession + 0.0018*fabric + 0.0017*gauge + 0.0017*affect + 0.0016*visible + 0.0016*exhale + 0.0016*halfway\n",
      "\n",
      "Probability displayed/total = 0.047395/1.000000\n",
      "\n",
      "7:  0.0067*brain + 0.0064*ridge + 0.0041*dahn + 0.0029*centre + 0.0022*stimulate + 0.0022*lousy + 0.0018*commercial + 0.0018*strip + 0.0018*tapping + 0.0017*chakra + 0.0017*eves + 0.0017*abdomen + 0.0016*media + 0.0016*gong + 0.0016*chi + 0.0016*tai + 0.0015*practioner + 0.0015*tues + 0.0015*insult + 0.0015*behaving\n",
      "\n",
      "Probability displayed/total = 0.047261/1.000000\n",
      "\n",
      "8:  0.0033*journal + 0.0029*chart + 0.0024*bodywork + 0.0023*artists + 0.0021*straining + 0.0021*culture + 0.0020*steal + 0.0020*magical + 0.0020*journey + 0.0019*edge + 0.0019*comfy + 0.0018*choreographed + 0.0018*warrior + 0.0018*secrets + 0.0017*son + 0.0016*translates + 0.0016*evaluations + 0.0016*finest + 0.0016*empathetic + 0.0015*resolution\n",
      "\n",
      "Probability displayed/total = 0.039841/1.000000\n",
      "\n",
      "9:  0.0053*ishta + 0.0030*pregnancy + 0.0030*rehearsed + 0.0023*postnatal + 0.0020*freestyle + 0.0020*daughter + 0.0019*tai + 0.0019*cup + 0.0018*trek + 0.0017*forest + 0.0017*january + 0.0016*march + 0.0016*certification + 0.0015*birthday + 0.0015*exposure + 0.0015*mediation + 0.0015*semi + 0.0015*interactions + 0.0014*miraculously + 0.0014*oils\n",
      "\n",
      "Probability displayed/total = 0.040300/1.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Print out the topics, with their term distributions.\n",
    "'''\n",
    "num_topics   = n_LDA_topics\n",
    "NYC_lda_file = 'results/yoga_studios_nyc_lda'+str(num_topics)\n",
    "NYC_lda = models.LdaModel.load(NYC_lda_file)\n",
    "for itopic in range(num_topics):\n",
    "    out_string = \"{:d}\".format(itopic)+\": \"\n",
    "    pshown     = 0.0\n",
    "    for ind,(prob,word_stem) in enumerate(NYC_lda.show_topic(itopic, topn=20)):\n",
    "        word    = NYC_stem_to_word[word_stem]\n",
    "        pshown += prob\n",
    "        if word == \"\":\n",
    "            word = \"[\"+word_stem+\"]\"\n",
    "        if ind > 0:\n",
    "            out_string += \" + \"+\"{:.4f}\".format(prob)+\"*\"+word\n",
    "        else:\n",
    "            out_string += \" \"+\"{:.4f}\".format(prob)+\"*\"+word\n",
    "    print(out_string)    \n",
    "    ptotal = sum([prob for (prob,word_stem) in NYC_lda.show_topic(itopic,topn=n_terms)])\n",
    "    print(\"\\nProbability displayed/total = %f/%f\\n\" % (pshown,ptotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "[Back to top](#section1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze LA Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening LA database...\n",
      "Total number of Yoga businesses = 749\n",
      "Business Runyon Canyon Park has 1285 reviews\n",
      "Business Gold?s Gym Downtown Los Angeles has 380 reviews\n",
      "Business 24 Hour Fitness has 426 reviews\n",
      "Business 24 Hour Fitness has 304 reviews\n",
      "Business LA Fitness has 317 reviews\n",
      "Number of reviewed LA Yoga businesses = 749\n",
      "Dictionary(5447 unique tokens: [u'haggl', u'ayurved', u'profici', u'pardon', u'ridden']...)\n",
      "In LA corpus: Number of terms = 5447, documents = 749, latent topics = 10\n",
      "CPU times: user 8min 42s, sys: 20.1 s, total: 9min 2s\n",
      "Wall time: 10min 51s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "* Make a list of the LA reviews we'll be analyzing, concatenating by business.\n",
    "* Tokenize, and if enabled, remove stopwords and proper nouns, and stem.\n",
    "* Create a corpus dictionary: (integer word ID, word, word frequency in corpus),\n",
    "  removing words that appear too infrequently or too frequently.\n",
    "* Convert the tokenized reviews of the corpus to bags of words.\n",
    "* Apply tf-idf transformation to corpus.\n",
    "* Apply Latent Dirichlet Allocation to the corpus.\n",
    "'''\n",
    "\n",
    "# Initialize output lists:\n",
    "LA_names    = []\n",
    "LA_reviews  = []\n",
    "LA_ratings  = []\n",
    "LA_websites = []\n",
    "\n",
    "client = MongoClient()\n",
    "yoga   = client.dsbc.yyrla\n",
    "print('Opening LA database...')\n",
    "print('Total number of Yoga businesses = %i' %yoga.count())\n",
    "\n",
    "cursor = yoga.find()\n",
    "for record in cursor:\n",
    "    reviews = []\n",
    "    for review in record[\"usr_text\"]:\n",
    "        if review:\n",
    "            reviews.append(review)\n",
    "\n",
    "    # Eliminate duplicate reviews for a given studio\n",
    "    # (different studios may still \"share\" a review):\n",
    "    n_reviews = len(reviews)\n",
    "    ureviews  = []\n",
    "    for review in set(reviews):\n",
    "        ureviews.append(review)\n",
    "    n_ureviews = len(ureviews)\n",
    "    if n_ureviews > 300:\n",
    "        print('Business %s has %i reviews' %(record[\"biz_name\"],n_ureviews))\n",
    "    \n",
    "    # Concatenate the unique reviews by business.\n",
    "    con_review = \"\"\n",
    "    for review in ureviews:\n",
    "        con_review += \" \" + review\n",
    "\n",
    "    if con_review:\n",
    "        con_review += \" \" + record[\"biz_name\"] # Add business name string only if there are reviews\n",
    "        studio = record[\"biz_name\"]+\" [at] \"+record[\"biz_address\"]\n",
    "        LA_names.append(studio)\n",
    "        LA_reviews.append(con_review)\n",
    "        LA_ratings.append(record[\"biz_rating\"])\n",
    "        LA_websites.append(record[\"biz_website\"])\n",
    "\n",
    "print('Number of reviewed LA Yoga businesses = %i' % len(LA_reviews))\n",
    "\n",
    "# Tokenize, removing punctuation and numbers, and if enabled, convert to lower case and remove accents.\n",
    "LA_reviews_1 = [list(utils.tokenize(studio_review,lowercase=convert_lowercase,\n",
    "                deacc=remove_accents)) for studio_review in LA_reviews]\n",
    "\n",
    "if remove_stopwords:\n",
    "    LA_reviews_2 = [[word for word in studio_review if word not in stoplist] \n",
    "                    for studio_review in LA_reviews_1]\n",
    "    LA_reviews_1 = LA_reviews_2[:]\n",
    "\n",
    "if remove_proper:\n",
    "    LA_reviews_2 = [[word for word in studio_review if word not in ppn] \n",
    "                    for studio_review in LA_reviews_1]\n",
    "    LA_reviews_1 = LA_reviews_2[:]\n",
    "\n",
    "if stem_tokens:\n",
    "    LA_reviews_2 = [[stemmer.stem(word) for word in studio_review] for studio_review in LA_reviews_1]\n",
    "    # Create a dictionary to map stems to words (this is a one-to-many map, but this shouldn't matter much).\n",
    "    LA_stem_to_word = defaultdict(str)\n",
    "    for review in LA_reviews_1:\n",
    "        for word in review:\n",
    "            word_stem = stemmer.stem(word)\n",
    "            LA_stem_to_word[word_stem] = word    \n",
    "    LA_reviews_1 = LA_reviews_2[:]\n",
    "else:\n",
    "    LA_stem_to_word = defaultdict(str)\n",
    "    for review in LA_reviews_1:\n",
    "        for word in review:\n",
    "            LA_stem_to_word[word] = word\n",
    "\n",
    "LA_dictionary = corpora.Dictionary( LA_reviews_1 )\n",
    "if filter_tails:\n",
    "    LA_dictionary.filter_extremes( no_below=filter_low, no_above=filter_up, keep_n=None )\n",
    "print(LA_dictionary)\n",
    "n_terms = len(LA_dictionary)\n",
    "n_docs  = len(LA_reviews_1)\n",
    "print('In LA corpus: Number of terms = %i, documents = %i, latent topics = %i' % (n_terms,n_docs,n_LDA_topics))\n",
    "\n",
    "LA_corpus_bow = [LA_dictionary.doc2bow(studio_review) for studio_review in LA_reviews_1]\n",
    "\n",
    "LA_tfidf = models.TfidfModel(LA_corpus_bow)\n",
    "LA_corpus_tfidf = LA_tfidf[LA_corpus_bow]\n",
    "\n",
    "LA_lda_file = 'results/yoga_studios_la_lda'+str(n_LDA_topics)\n",
    "%time LA_lda = models.LdaModel( LA_corpus_tfidf, id2word=LA_dictionary, num_topics=n_LDA_topics, \\\n",
    "                                update_every=5, passes=100 )\n",
    "LA_lda.save(LA_lda_file)\n",
    "    \n",
    "LA_corpus_lda = LA_lda[LA_corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  0.0026*awakened + 0.0023*chakras + 0.0022*devoted + 0.0021*whale + 0.0019*finest + 0.0019*elongated + 0.0016*unrelenting + 0.0015*hotel + 0.0015*remotely + 0.0014*breathwork + 0.0014*reiki + 0.0012*philosophical + 0.0012*embarked + 0.0012*poise + 0.0012*reccommend + 0.0012*hint + 0.0012*regimented + 0.0012*lessened + 0.0011*couch + 0.0011*gymnastics\n",
      "\n",
      "Probability displayed/total = 0.030822/1.000000\n",
      "\n",
      "1:  0.0111*thai + 0.0080*acupuncture + 0.0064*prenatal + 0.0055*masseuse + 0.0048*knots + 0.0043*birth + 0.0040*tissues + 0.0036*lake + 0.0036*reiki + 0.0036*ashtanga + 0.0035*corepower + 0.0031*healers + 0.0031*insurance + 0.0028*silver + 0.0028*iyengar + 0.0027*anatomy + 0.0026*cpy + 0.0026*swedish + 0.0025*brick + 0.0023*rehab\n",
      "\n",
      "Probability displayed/total = 0.082893/1.000000\n",
      "\n",
      "2:  0.0044*kundalini + 0.0037*pole + 0.0035*chiropractor + 0.0029*aerial + 0.0027*chi + 0.0027*dahn + 0.0025*lululemon + 0.0024*fusion + 0.0023*chakras + 0.0022*tai + 0.0020*silk + 0.0020*hatha + 0.0019*holistic + 0.0019*circuit + 0.0019*glendale + 0.0018*yas + 0.0018*ocean + 0.0017*acupuncturist + 0.0017*blue + 0.0016*compassionate\n",
      "\n",
      "Probability displayed/total = 0.047523/1.000000\n",
      "\n",
      "3:  0.0020*kwon + 0.0020*inglewood + 0.0019*technology + 0.0018*hummingbirds + 0.0017*harmonies + 0.0015*flew + 0.0015*accelerate + 0.0014*samba + 0.0014*iyengar + 0.0014*thrown + 0.0012*invented + 0.0012*derek + 0.0012*infants + 0.0012*shakti + 0.0011*accessories + 0.0010*arcadia + 0.0010*maze + 0.0009*highlight + 0.0009*certificates + 0.0009*handstand\n",
      "\n",
      "Probability displayed/total = 0.027255/1.000000\n",
      "\n",
      "4:  0.0037*krav + 0.0031*maga + 0.0026*retreat + 0.0018*divine + 0.0016*buti + 0.0015*da + 0.0015*psychological + 0.0015*inversions + 0.0013*illuminates + 0.0012*hula + 0.0012*unlock + 0.0012*getaway + 0.0012*spanish + 0.0011*twins + 0.0011*tricky + 0.0011*uncle + 0.0010*hoops + 0.0010*breakfast + 0.0010*perceptibly + 0.0010*vouch\n",
      "\n",
      "Probability displayed/total = 0.030745/1.000000\n",
      "\n",
      "5:  0.0038*radiant + 0.0036*bodyworks + 0.0026*clay + 0.0018*component + 0.0018*king + 0.0018*buti + 0.0016*mar + 0.0016*onsite + 0.0016*utmost + 0.0016*temple + 0.0015*incoming + 0.0015*sf + 0.0015*hood + 0.0014*ac + 0.0014*laptop + 0.0013*cafe + 0.0013*fitter + 0.0013*ayurvedic + 0.0013*homemade + 0.0013*cuz\n",
      "\n",
      "Probability displayed/total = 0.035415/1.000000\n",
      "\n",
      "6:  0.0059*reformer + 0.0044*physique + 0.0033*martial + 0.0030*courts + 0.0029*yogaworks + 0.0028*ymca + 0.0027*carpet + 0.0023*steam + 0.0023*trx + 0.0018*basketball + 0.0017*tvs + 0.0016*socks + 0.0016*crunched + 0.0016*garden + 0.0016*brentwood + 0.0016*adults + 0.0015*warrior + 0.0015*racks + 0.0015*jenn + 0.0015*defense\n",
      "\n",
      "Probability displayed/total = 0.047028/1.000000\n",
      "\n",
      "7:  0.0032*sacred + 0.0019*concert + 0.0018*vision + 0.0017*paint + 0.0016*effortless + 0.0015*corps + 0.0014*wealth + 0.0014*reiki + 0.0013*tibetan + 0.0013*fluid + 0.0012*mantra + 0.0012*strict + 0.0012*wasnt + 0.0011*lifesaver + 0.0011*repetitive + 0.0010*excluding + 0.0010*specialist + 0.0010*church + 0.0010*lexi + 0.0010*affected\n",
      "\n",
      "Probability displayed/total = 0.027876/1.000000\n",
      "\n",
      "8:  0.0020*knack + 0.0019*magician + 0.0019*fundamentals + 0.0018*prenatal + 0.0018*caroline + 0.0017*dome + 0.0016*transcends + 0.0015*nam + 0.0015*interpret + 0.0014*seamlessly + 0.0014*snake + 0.0014*sacred + 0.0014*manger + 0.0014*healers + 0.0013*brilliant + 0.0012*trending + 0.0012*aromatherapy + 0.0011*eagerly + 0.0011*height + 0.0011*meticulous\n",
      "\n",
      "Probability displayed/total = 0.029770/1.000000\n",
      "\n",
      "9:  0.0025*revolution + 0.0019*illegal + 0.0017*bonnie + 0.0016*warrior + 0.0015*grocery + 0.0014*realign + 0.0014*el + 0.0014*blazing + 0.0013*district + 0.0013*saddles + 0.0011*subtle + 0.0011*sublime + 0.0011*contagious + 0.0011*ca + 0.0011*afro + 0.0011*tribal + 0.0010*lounge + 0.0010*slouched + 0.0009*knack + 0.0009*nuance\n",
      "\n",
      "Probability displayed/total = 0.026230/1.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Print out the topics, with their term distributions.\n",
    "'''\n",
    "num_topics  = n_LDA_topics\n",
    "LA_lda_file = 'results/yoga_studios_la_lda'+str(num_topics)\n",
    "LA_lda      = models.LdaModel.load(LA_lda_file)\n",
    "for itopic in range(num_topics):\n",
    "    out_string = \"{:d}\".format(itopic)+\": \"\n",
    "    pshown     = 0.0\n",
    "    for ind,(prob,word_stem) in enumerate(LA_lda.show_topic(itopic, topn=20)):\n",
    "        word    = LA_stem_to_word[word_stem]\n",
    "        pshown += prob\n",
    "        if word == \"\":\n",
    "            word = \"[\"+word_stem+\"]\"\n",
    "        if ind > 0:\n",
    "            out_string += \" + \"+\"{:.4f}\".format(prob)+\"*\"+word\n",
    "        else:\n",
    "            out_string += \" \"+\"{:.4f}\".format(prob)+\"*\"+word\n",
    "    print(out_string)    \n",
    "    ptotal = sum([prob for (prob,word_stem) in LA_lda.show_topic(itopic,topn=n_terms)])\n",
    "    print(\"\\nProbability displayed/total = %f/%f\\n\" % (pshown,ptotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "[Back to top](#section1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some unusual words found in the NYC and LA corpora:\n",
    "\n",
    "In New York City corpus:\n",
    "\n",
    "* alma = Nueva Alma studio\n",
    "* bonda = Bonda Yoga Studio\n",
    "* daya = Daya Yoga Studio\n",
    "* elahi = Elahi Yoga in the UES\n",
    "* hys = Harlem Yoga Studio\n",
    "* ikm = International Krav Maga\n",
    "* joschi = Joschi Body Bodega\n",
    "* krav maga = self-defense system developed for the military in Israel\n",
    "* mrg = MRG fitness studio in Staten Island\n",
    "* tenafly = borough in Bergen County, New Jersey\n",
    "* vdy = Brooklyn Vindhya Yoga\n",
    "* yith = Yoga in the (Jersey City) Heights\n",
    "\n",
    "In Los Angeles corpus:\n",
    "\n",
    "* cpy is \"Core Power Yoga\"\n",
    "* bar could be juice bar, or simply bar, or barely, barefoot,...\n",
    "* yas refers to \"Yoga And Spinning\", a fitness center that provides both yoga classes and indoor cycling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
