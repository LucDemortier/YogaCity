{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Hierarchical Dirichlet Process Analysis of Yoga Reviews\n",
    "\n",
    "Here we apply the following procedure:\n",
    " 1. Concatenate the reviews by yoga business, making sure there are no duplicate reviews for a given business.\n",
    " 2. Convert to lower case, remove accents, and tokenize, retaining only tokens with alphabetical characters.\n",
    " 3. Remove stop words and proper nouns.\n",
    " 4. Stem.\n",
    " 5. Create a corpus dictionary: (integer word ID, word, word frequency in corpus).\n",
    " 6. Remove tokens that appear too often or not often enough.\n",
    " 7. Convert each concatenated studio review into bag-of-words format: a list of (token ID, token count) 2-tuples.\n",
    " 8. Apply tf-idf transformation to corpus.\n",
    " 9. Apply Hierarchical Dirichlet Process algorithm to corpus.\n",
    " 10. Look at the resulting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "First get the packages we'll need.\n",
    "'''\n",
    "from   pymongo import MongoClient\n",
    "import logging\n",
    "import nltk\n",
    "from   gensim import corpora, models, similarities, matutils, utils\n",
    "from   collections import defaultdict\n",
    "from   pprint import pprint\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set region: NYC or LA.\n",
    "'''\n",
    "region = \"NYC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening NYC database...\n",
      "Total number of Yoga businesses = 796\n",
      "Number of reviewed Yoga businesses = 550\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Make a list of the reviews we'll be analyzing, concatenating by business.\n",
    "'''\n",
    "\n",
    "client = MongoClient()\n",
    "if region == \"NYC\":\n",
    "    yoga = client.dsbc.yyrnyc\n",
    "    print('Opening NYC database...')\n",
    "else:\n",
    "    yoga = client.dsbc.yyrla\n",
    "    print('Opening LA database...')\n",
    "    \n",
    "print('Total number of Yoga businesses = %i' %yoga.count())\n",
    "\n",
    "cursor          = yoga.find()\n",
    "studio_names    = []\n",
    "studio_reviews  = []\n",
    "studio_ratings  = []\n",
    "for record in cursor:\n",
    "    reviews = []\n",
    "    for review in record[\"usr_text\"]:\n",
    "        if review:\n",
    "            # Save review:\n",
    "            reviews.append(review)\n",
    "\n",
    "    # Eliminate duplicate reviews for a given studio\n",
    "    # (different studios may still \"share\" a review):\n",
    "    n_reviews = len(reviews)\n",
    "    ureviews  = []\n",
    "    for review in set(reviews):\n",
    "        ureviews.append(review)\n",
    "    n_ureviews = len(ureviews)\n",
    "    \n",
    "    # Concatenate the unique reviews by business.\n",
    "    con_review = \"\"\n",
    "    for review in ureviews:\n",
    "        con_review += \" \" + review\n",
    "\n",
    "    if con_review:\n",
    "        studio = record[\"biz_name\"]+\" [at] \"+record[\"biz_address\"]\n",
    "        studio_names.append(studio)\n",
    "        studio_reviews.append(con_review)\n",
    "        studio_ratings.append(record[\"biz_rating\"])\n",
    "        \n",
    "print('Number of reviewed Yoga businesses = %i' %len(studio_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of occurrences = 10\n",
      " \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Use this cell to search through reviews.\n",
    "'''\n",
    "item    = \"tsahi \"\n",
    "\n",
    "icount  = sum([review.lower().count(item) for review in studio_reviews])\n",
    "print(\"Total number of occurrences = %i\" % icount)\n",
    "print(\" \")\n",
    "\n",
    "num     = 0\n",
    "for review in studio_reviews:\n",
    "    num   += 1\n",
    "    icount = review.lower().count(item)\n",
    "    if icount>0 and num > 127:\n",
    "        print(\"Number of occurrences in review %i = %i\" % (num,icount))\n",
    "        print(\" \")\n",
    "        print('%i: %s' %(num,review))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Convert to lower case, remove accents, tokenize, remove stop words,\n",
    "remove proper nouns, and stem.\n",
    "Then make a dictionary to map stems into words (one word per stem).\n",
    "'''\n",
    "\n",
    "# Get list of stopwords.\n",
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "stoplist.append(u'\\u0027s')   # \"'s\" as in \"he's\"\n",
    "stoplist.append(u'n\\u0027t')  # \"n't\" as in \"he hasn't\"\n",
    "stoplist.append(u'\\u0027m')   # \"'m\" as in \"I'm\"\n",
    "stoplist.append(u'ya')        # as in \"you\"\n",
    "stoplist.append(u'\\u0027ve')  # \"'ve\" as in \"I've\"\n",
    "stoplist.append(u'also')\n",
    "stoplist.append(u've')\n",
    "stoplist.append(u'm')\n",
    "\n",
    "# Convert to lower case, remove accents, and tokenize (removing punctuation and numbers).\n",
    "studio_reviews_1 = [list(utils.tokenize(studio_review,lowercase=True,deacc=True)) for studio_review in studio_reviews]\n",
    "\n",
    "# Remove stop words.\n",
    "studio_reviews_2 = [[word for word in studio_review if word not in stoplist] for studio_review in studio_reviews_1]\n",
    "\n",
    "# Remove proper nouns.\n",
    "ppn = [\"aaron\", \"aarona\", \"abigail\", \"adam\", \"adelaide\", \"alice\", \"alicia\", \"amalia\", \"amanda\", \"andrea\", \n",
    "       \"angela\", \"angie\", \"anna\", \"annie\", \"anya\", \"ariel\", \"ash\", \"ashley\", \"audra\", \n",
    "       \"becker\", \"becky\", \"belle\", \"beverly\", \"bijorn\", \"bjorn\", \"brandon\", \"brian\", \n",
    "       \"caprice\", \"cara\", \"carla\", \"carlos\", \"carolyn\", \"cathy\", \"charlotte\", \"chris\", \n",
    "       \"christine\", \"claire\", \"connie\", \"corey\", \"courtney\", \n",
    "       \"dalton\", \"daniela\", \"davey\", \"david\", \"deborah\", \"deena\", \"diane\", \"dina\", \"dr\", \n",
    "       \"eddie\", \"edwin\", \"elaine\", \"ellen\", \"emily\", \"eric\", \"erica\", \"erik\", \"erika\", \"erin\", \"ezmy\", \n",
    "       \"fergus\", \n",
    "       \"gabriella\", \"gavin\", \"geralyn\", \"ghylian\", \"gina\", \"glenda\", \n",
    "       \"hannah\", \"heather\", \"heidy\", \"henry\", \"hermann\", \"hsiao\", \"hunt\", \n",
    "       \"ikaika\", \"ingrid\", \"ivette\", \n",
    "       \"jacqui\", \"jahaira\", \"james\", \"jane\", \"janet\", \"jen\", \"jeni\", \"jenni\", \"jennie\", \"jennifer\", \"jenny\", \n",
    "       \"jess\", \"jesse\", \"jessica\", \"jill\", \"jillian\", \"jim\", \"joe\", \"joetta\", \"jose\", \n",
    "       \"joy\", \"joyce\", \"jq\", \"judy\", \"julia\", \"juliana\", \"julie\", \n",
    "       \"kalie\", \"kallie\", \"karen\", \"kathleen\", \"katie\", \"kaurwar\", \"ken\", \"kerri\", \"kerry\", \n",
    "       \"lalita\", \"lani\", \"lara\", \"lauren\", \"laurie\", \"liliana\", \"lindsay\", \"lindsey\", \n",
    "       \"lisa\", \"liz\", \"lori\", \"luisa\", \"lynn\", \n",
    "       \"madalina\", \"maggie\", \"malaika\", \"mandy\", \"marco\", \"margaret\", \"marja\", \"mark\", \"martha\", \"masako\", \"mayuri\", \n",
    "       \"meagan\", \"megan\", \"melissa\", \"melody\", \"meriany\", \"merilynn\", \"mia\", \n",
    "       \"michael\", \"michelle\", \"mike\", \"mimi\", \"mollie\", \"molly\", \"monica\", \"monika\", \"morgan\", \n",
    "       \"namgyal\", \"naomi\", \"narisara\", \"nathaniel\", \"nick\", \"nicola\", \"nicole\", \"nikki\", \"novak\",\n",
    "       \"paula\", \"pauline\", \"politeia\", \n",
    "       \"rachel\", \"rafael\", \"ramit\", \"rebeca\", \"rebecca\", \"rob\", \"roger\", \"rosie\", \"ruthie\", \"ryan\", \n",
    "       \"sandhya\", \"santoshi\", \"sara\", \"sarah\", \"shelly\", \"sherica\", \"sheryl\", \"sonja\", \"spencer\", \"stacey\", \"stacy\", \n",
    "       \"stephan\", \"stephanie\", \"stephaine\", \"steve\", \"sue\", \"susan\", \"suzanne\", \"suzi\", \"suzie\", \n",
    "       \"tzaki\", \"tsewang\", \"wayne\", \"wesley\", \"zander\" ]\n",
    "studio_reviews_2a = [[word for word in studio_review if word not in ppn] for studio_review in studio_reviews_2]\n",
    "\n",
    "# Stem.\n",
    "#stemmer          = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "stemmer          = nltk.stem.porter.PorterStemmer()\n",
    "studio_reviews_3 = [[stemmer.stem(word) for word in studio_review] for studio_review in studio_reviews_2a]\n",
    "\n",
    "# Create a dictionary to map stems to words (this is a one-to-many map, but this shouldn't matter much).\n",
    "stem_to_word = defaultdict(str)\n",
    "for studio_review in studio_reviews_2a:\n",
    "    for word in studio_review:\n",
    "        word_stem = stemmer.stem(word)\n",
    "        stem_to_word[word_stem] = word\n",
    "\n",
    "#pprint(studio_reviews_3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(15375 unique tokens: [u'fawn', u'foodi', u'childern', u'yellow', u'interchang']...)\n",
      "\n",
      "Stem \"acroyoga\" maps to word \"acroyoga\"\n",
      "Stem \"fanci\" maps to word \"fancy\"\n",
      "Stem \"enchantingli\" maps to word \"enchantingly\"\n",
      "Stem \"scold\" maps to word \"scolded\"\n",
      "Stem \"timberlak\" maps to word \"timberlake\"\n",
      "Stem \"starsi\" maps to word \"starsi\"\n",
      "Stem \"tuckaho\" maps to word \"tuckahoe\"\n",
      "Stem \"lord\" maps to word \"lord\"\n",
      "Stem \"starse\" maps to word \"starseed\"\n",
      "Stem \"desensit\" maps to word \"desensitize\"\n",
      "\n",
      "Length of dictionary = 15375\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create a corpus dictionary: (integer word ID, word, word frequency in corpus).\n",
    "Remove words that appear too infrequently or too frequently.\n",
    "\n",
    "Load dictionary if it already exists on disk.\n",
    "'''\n",
    "\n",
    "make_dict = True\n",
    "\n",
    "if region == \"NYC\":\n",
    "    fname1 = \"yoga_studios_nyc.dict\"\n",
    "    fname2 = \"yoga_studios_nyc_txt.dict\"\n",
    "else:\n",
    "    fname1 = \"yoga_studios_la.dict\"\n",
    "    fname2 = \"yoga_studios_la_txt.dict\"\n",
    "    \n",
    "if make_dict:\n",
    "    dictionary = corpora.Dictionary( studio_reviews_3 )\n",
    "    dictionary.filter_extremes( no_below=1, no_above=0.7, keep_n=None )\n",
    "    dictionary.save( fname1 )\n",
    "    dictionary.save_as_text( fname2, sort_by_word=False )\n",
    "else:\n",
    "    dictionary = corpora.Dictionary.load( fname1 )\n",
    "    \n",
    "print(dictionary)\n",
    "print(\"\")\n",
    "for i in range(10,20):\n",
    "    print('Stem \"%s\" maps to word \"%s\"' % (dictionary[i],stem_to_word[dictionary[i]]))\n",
    "print(\"\")\n",
    "print('Length of dictionary = %i' % len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(550 documents, 15375 features, 220500 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Convert the tokenized reviews of the corpus to bags of words,\n",
    "or load the corpus from disk if it already exists.\n",
    "'''\n",
    "make_bow = True\n",
    "\n",
    "if region == \"NYC\":\n",
    "    fname3 = \"yoga_studios_nyc.mm\"\n",
    "else:\n",
    "    fname3 = \"yoga_studios_la.mm\"\n",
    "    \n",
    "if make_bow:\n",
    "    # Convert each concatenated studio review into bag-of-words format: a list of (token ID, token count) 2-tuples.\n",
    "    corpus_bow = [dictionary.doc2bow(studio_review) for studio_review in studio_reviews_3]\n",
    "\n",
    "    # Store to disk in Matrix Market format (= a text format)\n",
    "    corpora.MmCorpus.serialize( fname3, corpus_bow )\n",
    "\n",
    "# Read bags-of-words corpus from disk.\n",
    "corpus_bow = corpora.MmCorpus( fname3 )\n",
    "\n",
    "print(corpus_bow)\n",
    "#print(corpus_bow[0]) # retrieving first document (for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Apply tf-idf transformation to corpus: Compute document frequencies of all the features.\n",
    "'''\n",
    "tfidf = models.TfidfModel(corpus_bow)\n",
    "corpus_tfidf = tfidf[corpus_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.49 s, sys: 639 ms, total: 7.13 s\n",
      "Wall time: 6.5 s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Apply Hierarchical Dirichlet Process analysis to the corpus.\n",
    "The following are arguments to HdpModel, with a brief description:\n",
    "    gamma:      [1]    first level concentration\n",
    "    alpha:      [1]    second level concentration\n",
    "    eta:        [0.01] the topic Dirichlet\n",
    "    T:          [150]  top level truncation level\n",
    "    K:          [15]   second level truncation level\n",
    "    kappa:      [1.0]  learning rate\n",
    "    tau:        [64.0] slow down parameter\n",
    "    max_time:   [None] stop training after this many seconds\n",
    "    max_chunks: [None] stop after having processed this many chunks (wrap around\n",
    "                       corpus beginning in another corpus pass, if there are not \n",
    "                       enough chunks in the corpus)\n",
    "    chunksize:  [256]  Training proceeds in chunks of `chunksize` documents at a time. \n",
    "                       The size of chunksize is a tradeoff between increased speed \n",
    "                       (bigger chunksize) and lower memory footprint (smaller chunksize). \n",
    "'''\n",
    "if region == \"NYC\":\n",
    "    hdp_file = 'results/yoga_studios_nyc_hdp'\n",
    "else:\n",
    "    hdp_file = 'results/yoga_studios_la_hdp'\n",
    "    \n",
    "make_hdp = True\n",
    "if make_hdp:\n",
    "    %time hdp = models.HdpModel( corpus_bow, id2word=dictionary, \\\n",
    "                                 alpha=1.0, gamma=1.0, T=150, K=15, kappa=0.1, eta=0.05 )\n",
    "    hdp.save(hdp_file)\n",
    "else:\n",
    "    hdp = models.HdpModel.load(hdp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics = 150, Sum of topic probabilities = 1.000000\n",
      "\n",
      "Topic 000 (0.222) : 0.01088*instructors + 0.00970*place + 0.00915*like + 0.00893*teachers + 0.00763*practically\n",
      "Topic 001 (0.202) : 0.00979*place + 0.00836*instructors + 0.00798*teachers + 0.00774*like + 0.00605*work\n",
      "Topic 002 (0.198) : 0.01594*gym + 0.00816*like + 0.00703*work + 0.00661*rooms + 0.00656*instructors\n",
      "Topic 003 (0.189) : 0.01063*instructors + 0.01005*place + 0.00798*like + 0.00720*practically + 0.00709*teachers\n",
      "Topic 004 (0.039) : 0.01220*place + 0.01214*yogi + 0.01113*hapi + 0.00820*teachers + 0.00769*never\n",
      "Topic 005 (0.021) : 0.00981*teachers + 0.00884*instructors + 0.00785*like + 0.00739*place + 0.00631*practically\n",
      "Topic 006 (0.020) : 0.00807*practically + 0.00770*instructors + 0.00675*teachers + 0.00659*like + 0.00634*place\n",
      "Topic 007 (0.016) : 0.00955*like + 0.00845*instructors + 0.00788*teachers + 0.00760*place + 0.00693*rooms\n",
      "Topic 008 (0.014) : 0.00855*teachers + 0.00833*like + 0.00726*instructors + 0.00671*place + 0.00612*practically\n",
      "Topic 009 (0.007) : 0.00912*instructors + 0.00696*like + 0.00585*workout + 0.00553*music + 0.00478*cycle\n",
      "Topic 010 (0.005) : 0.01286*bikram + 0.00759*instructors + 0.00721*like + 0.00678*teachers + 0.00634*rooms\n",
      "Topic 011 (0.004) : 0.01064*teachers + 0.00711*like + 0.00530*rooms + 0.00493*good + 0.00492*practically\n",
      "Topic 012 (0.004) : 0.00754*like + 0.00706*teachers + 0.00698*instructors + 0.00623*ishta + 0.00619*place\n",
      "Topic 013 (0.004) : 0.00708*like + 0.00633*hot + 0.00625*instructors + 0.00550*moksha + 0.00532*place\n",
      "Topic 014 (0.004) : 0.00661*bikram + 0.00628*like + 0.00621*rooms + 0.00612*instructors + 0.00503*place\n",
      "Topic 015 (0.003) : 0.00388*place + 0.00301*teachers + 0.00292*experience + 0.00291*practically + 0.00264*first\n",
      "Topic 016 (0.003) : 0.00782*instructors + 0.00761*spin + 0.00588*bikes + 0.00498*like + 0.00481*place\n",
      "Topic 017 (0.003) : 0.00668*like + 0.00564*pure + 0.00491*place + 0.00485*instructors + 0.00472*teachers\n",
      "Topic 018 (0.002) : 0.00934*massage + 0.00828*rooms + 0.00820*like + 0.00792*spa + 0.00613*core\n",
      "Topic 019 (0.002) : 0.00448*teachers + 0.00423*jai + 0.00394*practically + 0.00373*first + 0.00335*like\n",
      "Topic 020 (0.002) : 0.00850*gym + 0.00546*place + 0.00357*always + 0.00357*clean + 0.00342*teachers\n",
      "Topic 021 (0.002) : 0.00364*fit + 0.00346*trained + 0.00278*work + 0.00273*instructors + 0.00261*place\n",
      "Topic 022 (0.002) : 0.00580*bikram + 0.00558*like + 0.00546*clean + 0.00482*instructors + 0.00475*rooms\n",
      "Topic 023 (0.002) : 0.00273*place + 0.00185*teachers + 0.00184*work + 0.00184*back + 0.00184*something\n",
      "Topic 024 (0.002) : 0.00258*rooms + 0.00240*like + 0.00186*teachers + 0.00186*instructors + 0.00186*place\n",
      "Topic 025 (0.002) : 0.01366*massage + 0.00588*place + 0.00541*melting + 0.00386*recommend + 0.00374*like\n",
      "Topic 026 (0.001) : 0.00261*teachers + 0.00259*area + 0.00259*good + 0.00259*grace + 0.00259*people\n",
      "Topic 027 (0.001) : 0.00537*instructors + 0.00380*like + 0.00333*pilates + 0.00287*body + 0.00271*years\n",
      "Topic 028 (0.001) : 0.00201*teachers + 0.00182*new + 0.00180*space + 0.00179*experience + 0.00162*instructors\n",
      "Topic 029 (0.001) : 0.00935*massage + 0.00662*thai + 0.00326*session + 0.00310*recommend + 0.00229*would\n",
      "Topic 030 (0.001) : 0.00499*gym + 0.00261*would + 0.00244*fit + 0.00227*joined + 0.00210*see\n",
      "Topic 031 (0.001) : 0.00461*pilates + 0.00331*like + 0.00316*instructors + 0.00266*small + 0.00249*starting\n",
      "Topic 032 (0.001) : 0.00582*instructors + 0.00452*bikram + 0.00372*people + 0.00341*like + 0.00260*mat\n",
      "Topic 033 (0.001) : 0.00247*ashtanga + 0.00187*teachers + 0.00146*practically + 0.00126*know + 0.00107*helpful\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check topics found by HDP.\n",
    "'''\n",
    "\n",
    "topics_to_print = 150\n",
    "words_per_topic = 5\n",
    "\n",
    "alpha,beta = hdp.hdp_to_lda()\n",
    "print('Number of topics = %i, Sum of topic probabilities = %f' % (len(alpha),sum(alpha)))\n",
    "print(\"\")\n",
    "sorted_topics = sorted(zip(range(len(alpha)),list(alpha)),key=lambda x: -x[1])\n",
    "topics        = hdp.show_topics( -1, formatted=False )\n",
    "for topic_num in range(topics_to_print):\n",
    "    topic_index  = sorted_topics[topic_num][0]\n",
    "    topic_weight = sorted_topics[topic_num][1]\n",
    "    if topic_weight > 0.001:\n",
    "        out_string = \"Topic \"+\"{0:03d}\".format(topic_num)+\" (\"+\"{:.3f}\".format(topic_weight)+\") : \"\n",
    "        for ind,(word,weight) in enumerate(topics[topic_index][1]):\n",
    "            if ind == 0:\n",
    "                out_string += \"{:.5f}\".format(weight)+\"*\"+stem_to_word[word]\n",
    "            elif ind < words_per_topic:\n",
    "                out_string += \" + \"+\"{:.5f}\".format(weight)+\"*\"+stem_to_word[word]\n",
    "        print(out_string)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In New York City corpus:\n",
    "\n",
    "* alma = Nueva Alma studio\n",
    "* bonda = Bonda Yoga Studio\n",
    "* daya = Daya Yoga Studio\n",
    "* elahi = Elahi Yoga in the UES\n",
    "* hys = Harlem Yoga Studio\n",
    "* ikm = International Krav Maga\n",
    "* joschi = Joschi Body Bodega\n",
    "* krav maga = self-defense system developed for the military in Israel\n",
    "* mrg = MRG fitness studio in Staten Island\n",
    "* tenafly = borough in Bergen County, New Jersey\n",
    "* vdy = Brooklyn Vindhya Yoga\n",
    "* yith = Yoga in the (Jersey City) Heights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
